---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'LIMP: Learning Latent Shape Representations with Metric Preservation Priors'
subtitle: ''
summary: ''
authors:
- L. Cosmo
- A. Norelli
- O. Halimi
- R. Kimmel
- E. Rodolà
tags: []
categories: []
date: '2020-01-01'
lastmod: 2023-02-08T19:53:23+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-02-08T18:53:22.836666Z'
publication_types:
- '2'
abstract: In this paper, we advocate the adoption of metric preservation as a powerful
  prior for learning latent representations of deformable 3D shapes. Key to our construction
  is the introduction of a geometric distortion criterion, defined directly on the
  decoded shapes, translating the preservation of the metric on the decoding to the
  formation of linear paths in the underlying latent space. Our rationale lies in
  the observation that training samples alone are often insufficient to endow generative
  models with high fidelity, motivating the need for large training datasets. In contrast,
  metric preservation provides a rigorous way to control the amount of geometric distortion
  incurring in the construction of the latent space, leading in turn to synthetic
  samples of higher quality. We further demonstrate, for the first time, the adoption
  of differentiable intrinsic distances in the backpropagation of a geodesic loss.
  Our geometric priors are particularly relevant in the presence of scarce training
  data, where learning any meaningful latent structure can be especially challenging.
  The effectiveness and potential of our generative model is showcased in applications
  of style transfer, content generation, and shape completion. © 2020, Springer Nature
  Switzerland AG.
publication: '*Lecture Notes in Computer Science (including subseries Lecture Notes
  in Artificial Intelligence and Lecture Notes in Bioinformatics)*'
doi: 10.1007/978-3-030-58580-8_2
links:
- name: URL
  url: https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097812760&doi=10.1007%2f978-3-030-58580-8_2&partnerID=40&md5=bcb3c6e1ce83923803446a9ae09dcfe7
---
